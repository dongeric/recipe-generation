{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections, re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericdong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericdong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/ericdong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ericdong/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/ericdong/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/ericdong/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned-data.pkl', 'rb') as handle:\n",
    "    recipes = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Categories': ['Main dish', 'Pasta', 'Vegetables'],\n",
       " 'Yield': '6',\n",
       " 'Ingredients': ['8 oz Pasta (preferably linguine)',\n",
       "  '3 x  Carrots, thinly sliced',\n",
       "  '2 tb Safflower or Olive oil',\n",
       "  '3 x  Sm Zucchini, thinly sliced',\n",
       "  '1/4 lb Peapods',\n",
       "  'MMMMM---------------------------PESTO--------------------------------',\n",
       "  '2 c  Fresh Basil Leaves',\n",
       "  '1/4 c  Pine nuts (pignolli)',\n",
       "  '2 x  Cloves Garlic',\n",
       "  '1 tb Olive oil'],\n",
       " 'Instructions': \"PASTA GARNISH: freshly ground black pepper and Parmesan cheese,  optional '''''''''''''''''''''''''''''''''''''''''''''''''''''''  '''''''''''''''' PESTO: Place ingredients in bowl of food processor.  Process until smooth, using rubber scraper to push down the sides  occasionally. Makes 1/2 cup. VARIATIONS: - add 3/4 c freshly grated  Parmesan Cheese - subst. cream cheese, kefir, or Neufchatel cheese  for oil - subst. walnuts or hazelnuts for pine nuts PASTA: Boil a  large pot of water; cook pasta until al dente. While pasta is  cooking, prepare pesto; set aside, covered. Steam carrots. Meanwhile,  in skillet, heat oil. Add zucchini and peapods. Stir continuously  until crisp/tender. When pasta is done, drain well; toss pesto with  noodles until they are well coated. Then toss in vegetables. Garnish  with pepper and cheese. VARIATIONS: - add 1/2 c Parmesan cheese to  Pesto ~ add or substitute other steamed or sauteed vegetables such as  mushrooms, peas, or sweet red pepper.\"}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes['pasta al pesto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2976\n"
     ]
    }
   ],
   "source": [
    "food_words = ['food', 'cook', 'fruit', 'vegetable', 'meat', 'eat', 'taste', 'fat', 'dairy', 'grain', \n",
    "              'drink', 'beverage', 'alcohol', 'bread', 'flavor', 'spice', 'dessert']\n",
    "food_synsets = set([synset for food in food_words for synset in wordnet.synsets(food)])\n",
    "diff = len(food_synsets)\n",
    "\n",
    "while diff > 0:\n",
    "    hyponyms = set([hyponym for synset in food_synsets for hyponym in synset.hyponyms()])\n",
    "    old_len = len(food_synsets)\n",
    "    food_synsets = food_synsets.union(hyponyms)\n",
    "    new_len = len(food_synsets)\n",
    "    diff = new_len - old_len\n",
    "\n",
    "print(len(food_synsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_entities = ['ORGANIZATION', 'PERSON', 'LOCATION']\n",
    "\n",
    "def has_ne(instruction, relevant_entities):\n",
    "    instruction_tree = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(instruction)))\n",
    "    for subtree in instruction_tree.subtrees():\n",
    "        if subtree.label() in relevant_entities:\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for title, recipe in recipes.items():\n",
    "    if 'Categories' in recipe:\n",
    "        recipes[title]['Categories'] = [category.lower() for category in recipe['Categories']]\n",
    "    if 'Ingredients' in recipe:\n",
    "        ingredients = nltk.pos_tag([lemmatizer.lemmatize(token.lower()) for ingredient in recipe['Ingredients'] \n",
    "                                    for token in nltk.word_tokenize(ingredient)])\n",
    "        ingredients = [word for word, pos in ingredients if pos == 'NN' and \n",
    "                       not set(wordnet.synsets(word)).isdisjoint(food_synsets)]\n",
    "        recipes[title]['Ingredients'] = ingredients\n",
    "    if 'Instructions' in recipe:\n",
    "        instructions = nltk.sent_tokenize(recipe['Instructions'])\n",
    "        cleaned_instructions = [instruction.lower() for instruction in instructions \n",
    "                                if not has_ne(instruction, relevant_entities)]\n",
    "        recipes[title]['Instructions'] = cleaned_instructions\n",
    "    count += 1\n",
    "    if (count % 4500):\n",
    "        print('Processed %d recipes' % count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Categories': ['main dish', 'indian', 'vegetarian', 'curries'],\n",
       " 'Yield': '4',\n",
       " 'Ingredients': ['lentil',\n",
       "  'oil',\n",
       "  'onion',\n",
       "  'cumin',\n",
       "  'seed',\n",
       "  'clove',\n",
       "  'garlic',\n",
       "  'ginger',\n",
       "  'cayenne',\n",
       "  'plum',\n",
       "  'tomato',\n",
       "  'lemon',\n",
       "  'juice',\n",
       "  'c',\n",
       "  'cauliflower',\n",
       "  'cilantro',\n",
       "  'jalapeno',\n",
       "  'pepper',\n",
       "  'sugar'],\n",
       " 'Instructions': ['in a large saucepan over low heat, combine lentils, onions, curry  powder, salt, turmeric, and 2 cups water;  bring to a simmer.',\n",
       "  'cover  and cook, stirring occasionally, until the lentils are soft and the  sauce has thickened, about 45 minutes.',\n",
       "  'add tomatoes, cauliflower,  and jalapeno peppers and simmer, covered, until the cauliflower is  tender, 8 to 10 minutes longer.',\n",
       "  'remove from heat.',\n",
       "  'heat oil in a small skillet over medium-high heat.',\n",
       "  'add cumin seeds  and cook for about 10 seconds.',\n",
       "  'add garlic and ginger;  saute until  the garlic is lightly browned, about 1 minute.',\n",
       "  'stir in cayenne and  immediately add the oil-spice mixture to the cauliflower mixture.',\n",
       "  'stir in lemon juice, cilantro, and sugar.',\n",
       "  'taste and adjust  seasonings with additional salt and cayenne.',\n",
       "  'serve over rice.']}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes['cauliflower and red lentil curry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96361"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('processed-data.pkl', 'wb')\n",
    "pickle.dump(recipes, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
